---
jupyter:
  jupytext:
    text_representation:
      extension: .Rmd
      format_name: rmarkdown
      format_version: '1.2'
      jupytext_version: 1.14.7
  kernelspec:
    display_name: Python 3
    name: python3
---

<!-- #region id="8UTIGpTZZlOO" -->
# Imports
<!-- #endregion -->

```{python id="AnwSHO1L97I5"}
import datetime
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import xgboost as xgb
import geopandas as gpd

from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_absolute_percentage_error
from sklearn.model_selection import GridSearchCV
from sklearn.preprocessing import LabelEncoder
from sklearn.cluster import AgglomerativeClustering
from itertools import product
```

<!-- #region id="pSSMa3G2wGmF" -->
# Configs
<!-- #endregion -->

```{python id="zIkDLK9zwGmG"}
INPUT_PATH = 'datasets/'
OUTPUT_PATH = 'results/xgb_predictions.parquet'

RIDGE_TEST_PATH = 'results/ridge_test_predictions.parquet'
RIDGE_TRAIN_PATH = 'results/ridge_train_predictions.parquet'

START_DATE = '2023-01-01'
END_DATE = '2023-04-30'

FEATURE_LIST = [
                'Ridge_predict',

                'Previous_week_max_demand',
                'Previous_2week_max_demand',

                'Previous_2week_demand',
                'Previous_week_demand', 

                'Previous_week_group_max_demand',
                'Previous_week_group_min_demand',
                'Previous_week_group_mean_demand',

                'Previous_day_demand',
                'Previous_2day_demand',
                'Previous_3day_demand',
                'Previous_4day_demand',
                'Previous_5day_demand',
                'Previous_6day_demand',
                'Previous_8day_demand',
                'Previous_9day_demand',
                'Previous_10day_demand',
                'Previous_11day_demand',
                'Previous_12day_demand',
                'Previous_13day_demand',

                #  'Day_of_month', 
                #  'Day_of_week',
                ]

TEST_START_DATE = '2023-04-01'
VALIDATION_START_DATE = '2023-03-21'

GROUP_NUM = 3
AUTO_TUNE = False
```

<!-- #region id="5eS5BQE9wGmK" -->
# Data preparation

<!-- #endregion -->

<!-- #region id="TWsjXbpaznc_" -->
## Load Data
<!-- #endregion -->

```{python id="HAyNjqxNwGmH"}
def load_data(path, start_date: str, end_date: str):
    df = pd.read_parquet(path)
    start_date = datetime.date.fromisoformat(start_date)
    end_date = datetime.date.fromisoformat(end_date)
    filtered_df = df[(df['tpep_pickup_datetime'].dt.date >= start_date) &
                     (df['tpep_pickup_datetime'].dt.date <= end_date)]
    dataset = filtered_df.filter(items=['tpep_pickup_datetime', 'PULocationID'])
    dataset['PU_date'] = pd.to_datetime(dataset['tpep_pickup_datetime'].dt.date)
    return dataset
```

```{python id="87BFHUu1-z73"}
rides_df = load_data(INPUT_PATH, START_DATE, END_DATE)
```

```{python id="hCN-11QT3bp1"}
print(f'rides dataframe shape : {rides_df.shape}')
rides_df.head()
```

```{python}
ridge_train_df = pd.read_parquet(RIDGE_TRAIN_PATH)
ridge_test_df = pd.read_parquet(RIDGE_TEST_PATH)
ridge_df = pd.concat([ridge_train_df, ridge_test_df], axis = 0)
```

```{python}
print(f'ridge dataframe shape : {ridge_df.shape}')
ridge_df.head()
```

```{python}
regions_df = gpd.read_file('taxi_zones/taxi_zones.shp')
regions_df = regions_df.rename(columns = {'LocationID' : 'Location'})
```

<!-- #region id="X2ES_CY6-fb5" -->
## Labeling
<!-- #endregion -->

```{python id="a7mNMQ-zwGmH"}
def labeling(dataset, zones):
    dataset_labels = (
        dataset
        .groupby(['PULocationID', 'PU_date'])['PU_date']
        .count()
        .to_frame('Demand')
        .sort_values(['PULocationID', 'PU_date'], ascending=[True, True])
        .reset_index()
        .rename(columns={'PULocationID': 'Location', 'PU_date': 'Date'})
    )

    locations = pd.DataFrame(dataset_labels['Location'].unique(), columns=['Location'])
    dates = pd.DataFrame(dataset_labels['Date'].unique(), columns=['Date'])

    location_date_df = (
        locations
        .merge(dates, how='cross')
        .sort_values(['Location', 'Date'], ascending=[True, True])
        .reset_index(drop=True)
    )

    labels_df = (
        location_date_df
        .merge(dataset_labels, how='left', on=['Location', 'Date'])
        .fillna(value=0)
    )
    return labels_df

```

```{python}
regions_df['borough'] = LabelEncoder().fit_transform(regions_df[['borough', 'Location']]['borough'])
```

```{python id="VVRLakW_LeGp"}
labeled_df = labeling(rides_df, regions_df[['borough', 'Location']])
```

```{python id="0Gm5j5em28Xy"}
print(f'labeled dataframe shape : {labeled_df.shape}')
labeled_df.head()
```

```{python}
labeled_df.to_parquet('labels.parquet')
```

## Grouping Locations


### Agglomerative Clustering

```{python}
location_demand_df = labeled_df.groupby('Location')['Demand'].max().to_frame('Demand').sort_values(by = 'Demand').reset_index()
clustering = AgglomerativeClustering(n_clusters = GROUP_NUM).fit(location_demand_df['Demand'].to_numpy().reshape(-1, 1))
```

```{python}
plt.scatter(location_demand_df['Demand'], location_demand_df['Location'], c = clustering.labels_)
plt.show()
```

```{python}
def group_locations(location_demand_df, group_num):
    locations = [[] for i in range(group_num)]
    for i in range(group_num):
        demand_values = location_demand_df['Demand'].to_numpy().reshape(-1, 1)[clustering.labels_ == i]
        locations[i].append(
            location_demand_df[(location_demand_df['Demand'] >= np.min(demand_values))
            &(location_demand_df['Demand'] <= np.max(demand_values))]['Location'].values)
    
    return locations
```

```{python}
sorted_group_labels  = [i for i in range(GROUP_NUM)]
```

```{python}
grouped_locations = group_locations(location_demand_df, GROUP_NUM)
```

```{python}
grouped_locations_dfs = []
for i in range(GROUP_NUM):
    one_group_demand_df = grouped_locations[sorted_group_labels[i]][0]
    one_group_demand_df = labeled_df[labeled_df['Location'].isin(one_group_demand_df)].reset_index(drop = True)
    grouped_locations_dfs.append(one_group_demand_df)
```

<!-- #region id="G0W2pR-70JJb" -->
## Add Feature
<!-- #endregion -->

```{python id="ifvk6uWS6hnT"}
def feature_engineering(dataset):
    dataset['Previous_day_demand'] = dataset.groupby(['Location'])['Demand'].shift(1)
    dataset['Previous_2day_demand'] = dataset.groupby(['Location'])['Demand'].shift(2)
    dataset['Previous_3day_demand'] = dataset.groupby(['Location'])['Demand'].shift(3)
    dataset['Previous_4day_demand'] = dataset.groupby(['Location'])['Demand'].shift(4)
    dataset['Previous_5day_demand'] = dataset.groupby(['Location'])['Demand'].shift(5)
    dataset['Previous_6day_demand'] = dataset.groupby(['Location'])['Demand'].shift(6)
    dataset['Previous_week_demand'] = dataset.groupby(['Location'])['Demand'].shift(7)
    dataset['Previous_8day_demand'] = dataset.groupby(['Location'])['Demand'].shift(8)
    dataset['Previous_9day_demand'] = dataset.groupby(['Location'])['Demand'].shift(9)
    dataset['Previous_10day_demand'] = dataset.groupby(['Location'])['Demand'].shift(10)
    dataset['Previous_11day_demand'] = dataset.groupby(['Location'])['Demand'].shift(11)
    dataset['Previous_12day_demand'] = dataset.groupby(['Location'])['Demand'].shift(12)
    dataset['Previous_13day_demand'] = dataset.groupby(['Location'])['Demand'].shift(13)
    dataset['Previous_2week_demand'] = dataset.groupby(['Location'])['Demand'].shift(14)

    dataset['Previous_week_max_demand'] = dataset.groupby('Location')['Demand'].rolling(window=7).max().reset_index(drop = True)
    dataset['Previous_2week_max_demand'] = dataset.groupby('Location')['Demand'].rolling(window=14).max().reset_index(drop = True)

    data_per_date = []

    # Get the unique dates from the DataFrame
    unique_dates = dataset['Date'].unique()

    # Number of days to look back (in this case, 7 days)
    days_to_look_back = 7

    # Iterate through each date
    for date in unique_dates:
        # Filter the DataFrame to get the 7 past day rows for the current date
        past_7_days_df = dataset[dataset['Date'] >= date - pd.Timedelta(days=days_to_look_back)]
        past_7_days_df = past_7_days_df[past_7_days_df['Date'] < date]
        
        # Convert the filtered DataFrame to a dictionary with location as keys and demand values as values
        data_dict = past_7_days_df.set_index('Location')['Demand'].to_dict()
        data_per_date.append(data_dict)

    # Create an empty list to store the maximum demand for each date
    max_demand_per_date = []
    min_demand_per_date = []
    mean_demand_per_date = []

    # Iterate through the data_per_date list
    for data_dict in data_per_date:
        if data_dict:  # Check if the dictionary is not empty
            max_demand_per_date.append(max(data_dict.values()))
            min_demand_per_date.append(min(data_dict.values()))
            mean_demand_per_date.append(np.mean(list(data_dict.values())))
        else:
            max_demand_per_date.append(None)
            min_demand_per_date.append(None)
            mean_demand_per_date.append(None)

    # Convert the list to a numpy array
    max_demand_per_date = np.array(max_demand_per_date)
    min_demand_per_date = np.array(min_demand_per_date)
    mean_demand_per_date = np.array(mean_demand_per_date)

    max_demand_df = pd.DataFrame({'Date': unique_dates, 'Previous_week_group_max_demand': max_demand_per_date})
    min_demand_df = pd.DataFrame({'Date': unique_dates, 'Previous_week_group_min_demand': min_demand_per_date})
    mean_demand_df = pd.DataFrame({'Date': unique_dates, 'Previous_week_group_mean_demand': mean_demand_per_date})

    dataset = dataset.merge(max_demand_df, on='Date')
    dataset = dataset.merge(min_demand_df, on='Date')
    dataset = dataset.merge(mean_demand_df, on='Date')

    dataset['Day_of_week'] = dataset['Date'].dt.dayofweek   
    dataset['Day_of_month'] = dataset['Date'].dt.day
    
    return dataset
```

```{python}
grouped_locations_features_dfs = []
for i in range(GROUP_NUM):
    one_group_features_df = feature_engineering(grouped_locations_dfs[i])
    one_group_features_df = one_group_features_df.dropna().reset_index(drop = True)
    one_group_features_df['Previous_week_group_max_demand'] = one_group_features_df['Previous_week_group_max_demand'].astype('float64')
    one_group_features_df['Previous_week_group_min_demand'] = one_group_features_df['Previous_week_group_min_demand'].astype('float64')
    one_group_features_df['Previous_week_group_mean_demand'] = one_group_features_df['Previous_week_group_mean_demand'].astype('float64')
    grouped_locations_features_dfs.append(one_group_features_df)
```

```{python}
features_df = pd.concat(grouped_locations_features_dfs, axis = 0)

features_df = (
        features_df
        .merge(ridge_df, how='left', on=['Location', 'Date'])
        .rename(columns = {'Predicted_demand' : 'Ridge_predict'})
        )

features_df = features_df.sort_values(by = ['Location', 'Date']).reset_index(drop = True)
features_df = features_df
```

```{python id="bJxWEkiD3VOZ"}
print(f'features dataframe shape : {features_df.shape}')
features_df.head()
```

<!-- #region id="zN0kp6jw03DP" -->
## Split Train and Test Data
<!-- #endregion -->

```{python id="CMY1G1lmwGmI"}
def train_test_splitting(dataset, TEST_START_DATE):

    train_df = dataset[dataset['Date'] < TEST_START_DATE]
    test_df = dataset[dataset['Date'] >= TEST_START_DATE]

    return train_df, test_df
```

```{python id="3xH4VMGNwGmK"}
train_df, test_df = train_test_splitting(features_df, TEST_START_DATE)
```

```{python}
grid_search_train_df, valid_df = train_test_splitting(train_df, VALIDATION_START_DATE) 
```

```{python id="xxGdRZfqwGmL"}
print(f'train dataframe shape : {train_df.shape}')
train_df.head()
```

```{python}
print(f'validation dataframe shape : {valid_df.shape}')
valid_df.head()
```

```{python id="uPbINwH224Hy"}
print(f'test dataframe shape : {test_df.shape}')
test_df.head()
```

<!-- #region id="xf8ChW_7wGmL" -->
# Model Training
<!-- #endregion -->

<!-- #region id="mIhvw9lH92sa" -->
## **Gradient Boosting Regressor**
<!-- #endregion -->

<!-- #region id="13gFIyYfZyxh" -->
### Model Tuning
<!-- #endregion -->

```{python id="sOMPo5ryBm8g"}
def grid_search(model_class, param_grid, train_df, val_df, feature_list):
    best_params = None
    best_val_loss = float('inf')

    for params in product(*param_grid.values()):
        current_params = dict(zip(param_grid.keys(), params))
        current_model = model_class(**current_params)
        current_model.fit(train_df[feature_list], train_df['Demand'])
        
        y_val_pred = current_model.predict(val_df[feature_list])
        val_loss = mean_squared_error(val_df['Demand'], y_val_pred)
        
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            best_params = current_params
    
    return best_params, best_val_loss
```

```{python id="cm3YTQCMZVvu", outputId="4cf0ed3b-0243-476a-af87-35db760c9e5c"}
if AUTO_TUNE:
    params_test = {'learning_rate':[0.1], 
                'subsample':[0.5, 0.6, 0.7, 0.8], 
                'colsample_bytree':[0.7, 0.8, 0.9], 
                'max_depth':[4, 5, 6, 7, 8], 
                'min_child_weight':[5, 10, 20],
                }

    best_params, best_score = grid_search(
        model_class = xgb.XGBRegressor, 
        param_grid = params_test,
        train_df = grid_search_train_df, 
        val_df = valid_df,
        feature_list = FEATURE_LIST, 
        )
    
    print(best_params, best_score)
else:
    best_params = {
        'colsample_bytree': 0.8, 
        'learning_rate': 0.1, 
        'max_depth': 6, 
        'min_child_weight': 20, 
        'subsample': 0.9
        }
```

<!-- #region id="H0IHB8t41NB6" -->
### Prediction
<!-- #endregion -->

```{python id="pbhM5Oe6PjW7"}
def model_predict(model, train_data, test_data, feature_list):

    #model.fit(train_data[feature_list], train_data['Demand'])
    model.fit(train_data[feature_list], train_data['Demand'], eval_set=[(test_data[feature_list], test_data['Demand'])], verbose=True)
    print(np.min(model.evals_result()["validation_0"]["rmse"]))
    train_predict_df = model.predict(train_data[feature_list])
    test_predict_df = model.predict(test_data[feature_list])

    return train_predict_df, test_predict_df
```

```{python id="5AdQtdSqN7m9"}
model = xgb.XGBRegressor(**best_params)
train_prediction_df, test_prediction_df = model_predict(model, train_df, test_df, FEATURE_LIST)
```

<!-- #region id="J0EYl9KtTfo7" -->
### Visualization
<!-- #endregion -->

```{python id="ao6nw8xsRvB9"}
def prediction_visualization(train_data, test_data, train_prediction_df, test_prediction_df):

    train_data['Day_of_year'] = train_data['Date'].dt.dayofyear
    test_data['Day_of_year'] = test_data['Date'].dt.dayofyear

    predicted_train_df = train_data
    predicted_test_df = test_data
    predicted_train_df['Predicted'] = train_prediction_df
    predicted_test_df['Predicted'] = test_prediction_df

    train_data = train_data.groupby('Day_of_year')['Demand'].sum()
    test_data = test_data.groupby('Day_of_year')['Demand'].sum()
    predicted_train_df = predicted_train_df.groupby('Day_of_year')['Predicted'].sum()
    predicted_test_df = predicted_test_df.groupby('Day_of_year')['Predicted'].sum()

    plt.title('Train')
    plt.plot(train_data)
    plt.plot(predicted_train_df)
    plt.legend(["Real Value", "Predicted"], loc ="lower right")
    plt.show()

    plot_length = len(test_data)
    plt.title('Test')
    plt.plot(test_data)
    plt.plot(predicted_test_df)
    plt.legend(["Real Value", "Predicted"], loc ="lower right")
    plt.show()
```

```{python id="deyLWLmZThMJ", outputId="4d0094aa-099c-490f-aa82-281ee16bae01"}
prediction_visualization(train_df, test_df, train_prediction_df, test_prediction_df)
```

<!-- #region id="lERphf0kTist" -->
### Evaluation
<!-- #endregion -->

```{python id="BcGvcilUWEEC"}
def evaluate(metric, metric_name, true_values, predicted_values):
    print(f'{metric_name} : {metric(true_values, predicted_values)}')
```

```{python id="v4-GWghuSbnA"}
def evaluation(model_name, train_df, test_df, train_prediction_df, test_prediction_df):
    print(f'{model_name} train scores:')

    evaluate(mean_absolute_error, 'MAE', train_df['Demand'], train_prediction_df)
    evaluate(mean_squared_error, 'MSE', train_df['Demand'], train_prediction_df)
    evaluate(mean_absolute_percentage_error, 'MAPE', train_df['Demand'], train_prediction_df)

    print(f'\n{model_name} test scores:')

    evaluate(mean_absolute_error, 'MAE', test_df['Demand'], test_prediction_df)
    evaluate(mean_squared_error, 'MSE', test_df['Demand'], test_prediction_df)
    evaluate(mean_absolute_percentage_error, 'MAPE', test_df['Demand'], test_prediction_df)

```

```{python id="qSZwIFprTkqK", outputId="7ae94952-ec22-410a-9d26-6e05a96af6ab"}
evaluation('XGB', train_df, test_df, train_prediction_df, test_prediction_df)
```

<!-- #region id="_286hlGi7VWD" -->
### Feature Importance and SHAPE
<!-- #endregion -->

```{python id="osueYsNP1NB8", outputId="b3c13ee6-bd74-4470-a8c5-558df5416e28"}
xgb.plot_importance(model)
plt.show()
```

```{python id="50CEqbp75Y_i", outputId="b7288f1d-517a-4707-da22-ad996453f161"}
'''dtrain_reg = xgb.DMatrix(train_df[FEATURE_LIST].values, train_df['Demand'].values, enable_categorical=True)

explainer = shap.TreeExplainer(model)
shap_values = explainer.shap_values(dtrain_reg)
shap.summary_plot(shap_values, train_df[FEATURE_LIST])'''
```

# File Saving

```{python}
def save_predictions(dataset, path):
    dataset.to_parquet(path, index=False)
```

```{python}
def prediction_labeling(pred_df, labeled_df):
    pred_df = pd.DataFrame(pred_df, columns = ['Predicted_demand'])
    labeled_df.reset_index(inplace = True)
    labeled_prediction_df = labeled_df[['Location', 'Date']]
    labeled_prediction_df['Predicted_demand'] = pred_df
    return labeled_prediction_df
```

```{python}
labeled_prediction_df = prediction_labeling(test_prediction_df, test_df)
```

```{python}
print(f'labeled prediction dataframe shape : {labeled_prediction_df.shape}')
labeled_prediction_df.head()
```

```{python}
save_predictions(labeled_prediction_df, OUTPUT_PATH)
```
